{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directories if they don't exist\n",
    "!mkdir -p datasets\n",
    "!mkdir -p plot\n",
    "!mkdir -p model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download datasets\n",
    "\n",
    "Translation dataset used in original paper can be found [here](https://www.tensorflow.org/datasets/catalog/wmt14_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download english language word list\n",
    "if not os.path.exists('datasets/source_dictionary.txt'):\n",
    "    !cd datasets && curl -o source_dictionary.txt https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download french language word list\n",
    "if not os.path.exists('datasets/target_dictionary.tsv'):\n",
    "    !cd datasets && curl -O http://www.lexique.org/databases/Lexique383/Lexique383.zip\n",
    "    !cd datasets && unzip Lexique383.zip\n",
    "    !mv datasets/Lexique383.tsv datasets/target_dictionary.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download english-to-french translation dataset\n",
    "if not os.path.exists('datasets/translation.txt'):\n",
    "    !cd datasets && curl -O http://www.manythings.org/anki/fra-eng.zip\n",
    "    !cd datasets && unzip fra-eng.zip fra.txt\n",
    "    !mv datasets/fra.txt datasets/translation.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download english word embeddings\n",
    "if not os.path.exists('datasets/glove.6B.100d.txt'):\n",
    "    !cd datasets && curl -LO http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    !cd datasets && unzip glove.6B.zip glove.6B.100d.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Sentences from the source and destination languages are converted to token sequences and padded to the same length. Source sentences are padded on the left and destination sentences on the right. Special tokens representing words not in dicionary (\\<unk\\>), start of sequence (\\<sos\\>) and end of sequence (\\<eos\\>) are added to the destination sentences. There are two generated destination sentences, one for decoder input (which has the start token appended), and one for decoder ouput (which has the end token appended). Source language sequences are reversed for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word list for source language\n",
    "source_words = []\n",
    "\n",
    "# Load source language word list and convert characters to lower\n",
    "with open('datasets/source_dictionary.txt') as f:\n",
    "    source_words = f.read().lower().split('\\n')\n",
    "print(source_words[:20])\n",
    "\n",
    "# Make source language tokenizer. UNK token is 1\n",
    "source_tokenizer = Tokenizer(num_words=len(source_words), oov_token=1)\n",
    "source_tokenizer.fit_on_texts(source_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Word list for target language\n",
    "target_words = []\n",
    "\n",
    "# Load target language word list and convert characters to lower\n",
    "df = pd.read_csv('datasets/target_dictionary.tsv', sep='\\t', keep_default_na=False)['ortho'].tolist()\n",
    "target_words = [w.lower() for w in df]\n",
    "print(target_words[:20])\n",
    "\n",
    "# Make target language tokenizer. UNK token is 1\n",
    "target_tokenizer = Tokenizer(num_words=len(target_words), oov_token=1)\n",
    "target_tokenizer.fit_on_texts(target_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English-to-French translation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "UNK_TOKEN = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_sentences = []\n",
    "target_sentences_input = []\n",
    "target_sentences_output = []\n",
    "\n",
    "lines = []\n",
    "with open('datasets/translation.txt') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "NUM_LINES = 20_000\n",
    "for line in lines[:NUM_LINES]:\n",
    "    input_sentence, target_sentence, _ = line.split('\\t')\n",
    "    source_sentences.append(input_sentence)\n",
    "    target_sentences_input.append(SOS_TOKEN + ' ' + target_sentence)\n",
    "    target_sentences_output.append(target_sentence + ' ' + EOS_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NUM_SOURCE_WORDS = 50_000\n",
    "MAX_NUM_TARGET_WORDS = 50_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make source language tokenizer\n",
    "source_tokenizer = Tokenizer(num_words=MAX_NUM_SOURCE_WORDS)\n",
    "source_tokenizer.fit_on_texts(source_sentences)\n",
    "source_dict = source_tokenizer.word_index\n",
    "NUM_SOURCE_WORDS = len(source_dict)+1\n",
    "\n",
    "# Make destination language tokenizer\n",
    "# Don't filter '<' and '>' because they're used for SOS and EOS tokens\n",
    "target_tokenizer = Tokenizer(num_words=MAX_NUM_TARGET_WORDS, filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "target_tokenizer.fit_on_texts(target_sentences_input + target_sentences_output)\n",
    "target_dict = target_tokenizer.word_index\n",
    "NUM_TARGET_WORDS = len(target_dict)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(source_dict))\n",
    "print(len(target_dict))\n",
    "print(source_dict)\n",
    "print(target_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentences to sequences\n",
    "source_sequences = source_tokenizer.texts_to_sequences(source_sentences)\n",
    "target_sequences_input = target_tokenizer.texts_to_sequences(target_sentences_input)\n",
    "target_sequences_output = target_tokenizer.texts_to_sequences(target_sentences_output)\n",
    "\n",
    "# Reverse source sequnces\n",
    "for sequence in source_sequences:\n",
    "    sequence.reverse()\n",
    "\n",
    "max_source_len = max([len(sequence) for sequence in source_sequences])\n",
    "max_target_len = max([len(sequence) for sequence in target_sequences_output])\n",
    "\n",
    "encoder_input_sequences = pad_sequences(source_sequences, maxlen=max_source_len, padding='pre')\n",
    "decoder_input_sequences = pad_sequences(target_sequences_input, maxlen=max_target_len, padding='post')\n",
    "decoder_output_sequences = pad_sequences(target_sequences_output, maxlen=max_target_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_source_len)\n",
    "print(max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 10):\n",
    "    print(source_sentences[i])\n",
    "    print(str(encoder_input_sequences[i]))\n",
    "    print(target_sentences_input[i])\n",
    "    print(str(decoder_input_sequences[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common model for neural machine translation (NMT) is the sequence-to-sequence model which is a combination of recurrent neural network and encoder-decoder architectures. The enocder maps variable-length input sequence to a fixed-length representation, and the decoder uses that representation to generate an output sequence one word at a time.\n",
    "\n",
    "This type of model is used by Google for its translation service (https://arxiv.org/pdf/1609.08144.pdf, https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tokens is expected to be 1 greater than the number of words\n",
    "n_encoder_tokens = len(source_dict)+1\n",
    "n_decoder_tokens = len(target_dict)+1\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "LSTM_DIM = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer\n",
    "\n",
    "Keras documentation: https://keras.io/api/layers/core_layers/embedding/\n",
    "\n",
    "Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "\n",
    "e.g. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "\n",
    "This layer can only be used as the first layer in a model.\n",
    "\n",
    "Arguments:\n",
    "- **input_dim**: Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "- **output_dim**: Integer. Dimension of the dense embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM layer\n",
    "\n",
    "Keras documentation: https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "\n",
    "Long Short-Term Memory layer\n",
    "\n",
    "Arguments:\n",
    "- **units**: Positive integer, dimensionality of the output space.\n",
    "- **kernel_initializer**: Initializer for the kernel weights matrix\n",
    "- **return_sequences**: Boolean. Whether to return the last output in the output sequence, or the full sequence. Default: False.\n",
    "- **return_state**: Boolean. Whether to return the last state in addition to the output. Default: False.\n",
    "\n",
    "return_state returns the lstm output, last hidden state and last cell state and return_sequences returns hidden states for each step as the main output. Both flags can be used at the same time and will return all hidden states, last hidden state and last cell state.\n",
    "\n",
    "For more information on return_sequences and return_state read [this](https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer initializers\n",
    "\n",
    "Keras documentation: https://keras.io/api/layers/initializers/\n",
    "\n",
    "Initializers define the way to set the initial random weights of Keras layers.\n",
    "\n",
    "Some of the available initializers:\n",
    "- RandomNormal\n",
    "- RandomUniform\n",
    "- Zeros\n",
    "- Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe Embeddings\n",
    "\n",
    "Instead of learning the embeddings for english words during training, a pre-trained model called [GloVe](https://nlp.stanford.edu/projects/glove/) will be used. It has been trained on 6 billion words from [Wikipedia 2014](https://dumps.wikimedia.org/enwiki/latest/) and [Gigaworld 5](https://catalog.ldc.upenn.edu/LDC2011T07). The french word embeddings will be learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open('datasets/glove.6B.100d.txt', encoding=\"utf8\") as glove_file:\n",
    "    for line in glove_file:\n",
    "        record = line.split()\n",
    "        word = record[0]\n",
    "        word_embedding = np.asarray(record[1:], dtype='float32')\n",
    "        embeddings_dict[word] = word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((n_encoder_tokens, EMBEDDING_DIM))\n",
    "for word, index in source_dict.items():\n",
    "    embedding_vector = embeddings_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(embeddings_dict[\"hello\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(max_source_len,))\n",
    "encoder_embedding = Embedding(n_encoder_tokens, EMBEDDING_DIM, weights=[embedding_matrix], input_length=max_source_len)\n",
    "encoder_lstm = LSTM(\n",
    "    LSTM_DIM,\n",
    "    # kernel_initializer = tf.keras.initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None),\n",
    "    return_state=True\n",
    ")\n",
    "\n",
    "encoder = encoder_embedding(encoder_inputs)\n",
    "encoder, state_h, state_c = encoder_lstm(encoder)\n",
    "\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(max_target_len,))\n",
    "decoder_embedding = Embedding(n_decoder_tokens, EMBEDDING_DIM)\n",
    "decoder_lstm = LSTM(\n",
    "    LSTM_DIM, \n",
    "    # kernel_initializer = tf.keras.initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None),\n",
    "    return_sequences=True,\n",
    "    return_state=True\n",
    ")\n",
    "decoder_dense = Dense(n_decoder_tokens, activation='softmax')\n",
    "\n",
    "decoder = decoder_embedding(decoder_inputs)\n",
    "decoder, _, _ = decoder_lstm(decoder, initial_state=encoder_states)\n",
    "decoder = decoder_dense(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder)\n",
    "\n",
    "# It is possible to use beam search during training by definig a custom loss function\n",
    "# https://towardsdatascience.com/advanced-keras-constructing-complex-custom-losses-and-metrics-c07ca130a618\n",
    "\n",
    "# TODO Make adaptive learning rate using keras.callbacks.LearningRateScheduler\n",
    "# https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/\n",
    "# TODO Use SGD optimizer with no momentum, start rate 0.7\n",
    "# https://keras.io/api/optimizers/sgd/\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='plot/model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoder_input_sequences.shape)\n",
    "print(decoder_input_sequences.shape)\n",
    "print(decoder_output_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, encoder_input, decoder_input, decoder_output, n_classes, batch_size=32, shuffle=True):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.decoder_output = decoder_output\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    # Number of batches per epoch\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.encoder_input) / self.batch_size))\n",
    "\n",
    "    # Generate one batch\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X = [self.encoder_input[indexes], self.decoder_input[indexes]]\n",
    "        # Decoder outputs have to be one-hot-encoded\n",
    "        y = np.asarray([to_categorical(self.decoder_output[index], num_classes=self.n_classes) for index in indexes])\n",
    "        return X, y\n",
    "\n",
    "    # Update indexes for next epoch\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.encoder_input))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.arange(len(encoder_input_sequences))\n",
    "np.random.shuffle(indexes)\n",
    "\n",
    "validation_size = int(np.floor(len(encoder_input_sequences) * VALIDATION_SPLIT))\n",
    "\n",
    "train_indexes = indexes[:-validation_size]\n",
    "validation_indexes = indexes[-validation_size:]\n",
    "\n",
    "training_generator = DataGenerator(\n",
    "    encoder_input_sequences[train_indexes],\n",
    "    decoder_input_sequences[train_indexes],\n",
    "    decoder_output_sequences[train_indexes],\n",
    "    n_decoder_tokens,\n",
    "    BATCH_SIZE\n",
    ")\n",
    "\n",
    "validation_generator = DataGenerator(\n",
    "    encoder_input_sequences[validation_indexes],\n",
    "    decoder_input_sequences[validation_indexes],\n",
    "    decoder_output_sequences[validation_indexes],\n",
    "    n_decoder_tokens,\n",
    "    BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='model/checkpoint',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "history = model.fit(\n",
    "    training_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=[checkpoint_callback]\n",
    ")\n",
    "model.save(f'model/tranlator_{NUM_LINES}_{EPOCHS}_{BATCH_SIZE}_{VALIDATION_SPLIT}_{time.strftime(\"%Y%m%d_%H%M%S\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history['accuracy'])\n",
    "plt.plot(history.epoch, history.history['val_accuracy'])\n",
    "plt.legend(['Training accuracy', 'Validation accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save / Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model/translator_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model/translator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model/translator_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.load_model('model/translator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict outputs a separate model is needed because the previous decoder output has to be passed in as an input for the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_h = Input(shape=(LSTM_DIM,))\n",
    "decoder_state_c = Input(shape=(LSTM_DIM,))\n",
    "decoder_states = [decoder_state_h, decoder_state_c]\n",
    "\n",
    "decoder_input_word = Input(shape=(1,))\n",
    "\n",
    "inference_decoder = decoder_embedding(decoder_input_word)\n",
    "inference_decoder, state_h, state_c = decoder_lstm(inference_decoder, initial_state=decoder_states)\n",
    "inference_decoder = decoder_dense(inference_decoder)\n",
    "\n",
    "inference_states = [state_h, state_c]\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_input_word] + decoder_states,\n",
    "    [inference_decoder] + inference_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model.summary()\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(decoder_model, to_file='plot/inference_model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_source_dict = { v:k for k,v in source_dict.items() }\n",
    "reverse_target_dict = { v:k for k,v in target_dict.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_sequence(sentence):\n",
    "    sequence = source_tokenizer.texts_to_sequences([sentence])\n",
    "    sequence.reverse()\n",
    "    sequence = pad_sequences([sequence], maxlen=max_source_len, padding='pre')\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_sentence(sequence):\n",
    "    SOS_ID = target_dict[SOS_TOKEN]\n",
    "    EOS_ID = target_dict[EOS_TOKEN]\n",
    "    words = []\n",
    "    for i in sequence:\n",
    "        if i == SOS_ID:\n",
    "            continue\n",
    "        elif i == EOS_ID:\n",
    "            break\n",
    "        elif i > 0:\n",
    "            words.append(reverse_target_dict[i])\n",
    "        else:\n",
    "            words.append(UNK_TOKEN)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(source_sequence):\n",
    "    # Predict the encoder result directly\n",
    "    current_state = encoder_model.predict(source_sequence)\n",
    "    \n",
    "    # Predict ouput words one at a time until <eos> token or max_target_len\n",
    "    EOS_ID = target_dict[EOS_TOKEN]\n",
    "    \n",
    "    # Decoder_model expects a tensor as input\n",
    "    decoder_input = np.zeros((1, 1))\n",
    "    decoder_input[0, 0] = target_dict[SOS_TOKEN]\n",
    "    target_sequence = []\n",
    "    \n",
    "    for _ in range(max_target_len):\n",
    "        dense_outputs, state_h, state_c = decoder_model.predict([decoder_input] + current_state)\n",
    "        token = np.argmax(dense_outputs[0, 0, :])\n",
    "        if token == EOS_ID:\n",
    "            break\n",
    "        else:\n",
    "            target_sequence.append(token)\n",
    "        \n",
    "        # Replace decoder inputs to last generated token and states\n",
    "        decoder_input[0, 0] = token\n",
    "        current_state = [state_h, state_c]\n",
    "    return target_sequence\n",
    "\n",
    "def predict(source_sequence):\n",
    "    target_sequence = predict_sequence(source_sequence)\n",
    "    return sequence_to_sentence(target_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search\n",
    "\n",
    "The beam search algorithm is used as an improvement to the greedy search algorithm for determining the next word in the sequence. It works by taking the k best options at each step and then using those for all next steps. The best options are evalueted by maximizing the average log probability of each word is the output. For k=1 this is equivalent to the greedy algorithm. Increasing the k value gives better results but also increases processing time exponentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_predict_sequence(source_sequence, k=1):\n",
    "    encoder_output_state = encoder_model.predict([source_sequence])\n",
    "    decoder_input = np.zeros((1, 1))\n",
    "    \n",
    "    # (log(1), initial_sos_token, current_state)\n",
    "    k_beam = [(0, [target_dict[SOS_TOKEN]], encoder_output_state)]\n",
    "\n",
    "    EOS_ID = target_dict[EOS_TOKEN]\n",
    "    for i in range(max_target_len):\n",
    "        all_k_beams = []\n",
    "        for prob, predictions, state in k_beam:\n",
    "            if predictions[-1] == EOS_ID:\n",
    "                all_k_beams.append((prob, predictions, state))\n",
    "                continue\n",
    "            \n",
    "            decoder_input[0,0] = predictions[-1]\n",
    "            dense_outputs, state_h, state_c = decoder_model.predict([decoder_input] + state)\n",
    "            \n",
    "            # Get indices of top k predictions (last k when sorted)\n",
    "            top_k = dense_outputs[0,0].argsort()[-k:]\n",
    "\n",
    "            # Add to all possible candidates for k-beams\n",
    "            all_k_beams += [\n",
    "                (\n",
    "                    # We subtract the log because it's negative\n",
    "                    # Same as adding but sorting in reverse\n",
    "                    prob - np.log(dense_outputs[0,0,next_word]),\n",
    "                    # Append next word to the copy of existing list\n",
    "                    list(predictions)+[next_word],\n",
    "                    # Set state to new decoder state\n",
    "                    [state_h, state_c]\n",
    "                )\n",
    "                for next_word in top_k\n",
    "            ]\n",
    "\n",
    "        # Get k best tuples sorted by score/length\n",
    "        k_beam = sorted(all_k_beams, key = lambda t: t[0]/len(t[1]))[:k]\n",
    "\n",
    "    # Return best sequence\n",
    "    return k_beam[0][1]\n",
    "\n",
    "def beam_search_predict(source_sequence, k=1):\n",
    "    target_sequence = beam_search_predict_sequence(source_sequence, k)\n",
    "    return sequence_to_sentence(target_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 14261\n",
    "print(source_sentences[i])\n",
    "print(target_sentences_output[i])\n",
    "\n",
    "sample_input_sequences = [\n",
    "    encoder_input_sequences[i:i+1]\n",
    "]\n",
    "\n",
    "for seq in sample_input_sequences:\n",
    "    print(predict(seq))\n",
    "    print(beam_search_predict(seq, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground truth comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D Encoder hidden state PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Predict state for some sentences\n",
    "i = 0\n",
    "n = 5\n",
    "\n",
    "# Sample_sentences = [ ]\n",
    "sample_sentences = source_sentences[i:i+n+1]\n",
    "\n",
    "# TODO make make_sequence function\n",
    "source_sequences = source_tokenizer.texts_to_sequences(sample_sentences)\n",
    "# Reverse source sequnces\n",
    "for sequence in source_sequences:\n",
    "    sequence.reverse()\n",
    "sample_sequences = pad_sequences(source_sequences, maxlen=max_source_len, padding='pre')\n",
    "\n",
    "sample_inputs = encoder_model.predict(sample_sequences)[0]\n",
    "\n",
    "# print(sample_sentences)\n",
    "# print(sample_inputs)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(sample_inputs)\n",
    "pca_inputs = pca.transform(sample_inputs)\n",
    "\n",
    "plt.scatter(pca_inputs[:,0], pca_inputs[:,1])\n",
    "for i in range(len(sample_sentences)):\n",
    "    plt.text(pca_inputs[i,0], pca_inputs[i, 1], sample_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLEU score\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) is a score for comparing machine-translated text to one or more reference translations made by a human. Scoring usuall works on indiviual sentences and is then averaged on the entire text. Scores have a value between 0 and 1.\n",
    "\n",
    "For more info:\n",
    "- https://en.wikipedia.org/wiki/BLEU\n",
    "- https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
    "- https://towardsdatascience.com/bleu-bilingual-evaluation-understudy-2b4eab9bcfd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://arxiv.org/abs/1409.3215\n",
    "- https://arxiv.org/abs/1406.1078\n",
    "- https://arxiv.org/abs/1609.08144\n",
    "- https://keras.io/examples/nlp/lstm_seq2seq/\n",
    "- https://nlp.stanford.edu/projects/glove/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
