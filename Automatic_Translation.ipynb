{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ki_-MPUG59Xq"
   },
   "source": [
    "# Automatic Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YRVd4foW59X0"
   },
   "source": [
    "## Make workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1725,
     "status": "ok",
     "timestamp": 1617636268875,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "RvBQWzJS59X1"
   },
   "outputs": [],
   "source": [
    "# Make directories if they don't exist\n",
    "!mkdir -p datasets\n",
    "!mkdir -p plot\n",
    "!mkdir -p model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i68zQUJ959X2"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2836,
     "status": "ok",
     "timestamp": 1617636311385,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "7DbS-XzJ59X3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UlTOuYe59X3"
   },
   "source": [
    "## Download datasets\n",
    "\n",
    "Translation dataset used in original paper can be found [here](https://www.tensorflow.org/datasets/catalog/wmt14_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iIS8ggJN59X4"
   },
   "outputs": [],
   "source": [
    "# Download english language word list\n",
    "if not os.path.exists('datasets/source_dictionary.txt'):\n",
    "    !cd datasets && curl -o source_dictionary.txt https://raw.githubusercontent.com/dwyl/english-words/master/words_alpha.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LrozlrdE59X5"
   },
   "outputs": [],
   "source": [
    "# Download french language word list\n",
    "if not os.path.exists('datasets/target_dictionary.tsv'):\n",
    "    !cd datasets && curl -O http://www.lexique.org/databases/Lexique383/Lexique383.zip\n",
    "    !cd datasets && unzip Lexique383.zip\n",
    "    !mv datasets/Lexique383.tsv datasets/target_dictionary.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2018,
     "status": "ok",
     "timestamp": 1617636315666,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "vOZzRV8X59X6",
    "outputId": "b2241599-fab1-4e12-c583-18448fbcb624"
   },
   "outputs": [],
   "source": [
    "# Download english-to-french translation dataset\n",
    "if not os.path.exists('datasets/translation.txt'):\n",
    "    !cd datasets && curl -O http://www.manythings.org/anki/fra-eng.zip\n",
    "    !cd datasets && unzip fra-eng.zip fra.txt\n",
    "    !mv datasets/fra.txt datasets/translation.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 164282,
     "status": "ok",
     "timestamp": 1617636483339,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "VeJbqeNB59X6",
    "outputId": "650bfb04-bc94-4278-8fec-d4eccc4f75f2"
   },
   "outputs": [],
   "source": [
    "# Download english word embeddings\n",
    "if not os.path.exists('datasets/glove.6B.100d.txt'):\n",
    "    !cd datasets && curl -LO http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    !cd datasets && unzip glove.6B.zip glove.6B.100d.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use word lists from dictionary datasets or use words from translations\n",
    "USE_WORD_LIST = False\n",
    "# Use generators for loading data or load all at once\n",
    "# (Generators require less RAM but can be slower to train on)\n",
    "USE_DATA_GENERATORS = True\n",
    "# Reversing input sequences theoretically improves performace on longer sentences\n",
    "REVERSE_INPUT_SEQUENCES = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zak78JKz59X7"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "Sentences from the source and destination languages are converted to token sequences and padded to the same length. Source sentences are padded on the left and destination sentences on the right. Special tokens representing words not in dicionary (\\<unk\\>), start of sequence (\\<sos\\>) and end of sequence (\\<eos\\>) are added to the destination sentences. There are two generated destination sentences, one for decoder input (which has the start token appended), and one for decoder ouput (which has the end token appended). Source language sequences are reversed for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 681,
     "status": "ok",
     "timestamp": 1617636485275,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "qcKvGvbL59X8"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AzgTLvqn59X8"
   },
   "source": [
    "### English word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vc8y8PUc59X9"
   },
   "outputs": [],
   "source": [
    "if USE_WORD_LIST:\n",
    "    # Word list for source language\n",
    "    source_words = []\n",
    "\n",
    "    # Load source language word list and convert characters to lower\n",
    "    with open('datasets/source_dictionary.txt') as f:\n",
    "        source_words = f.read().lower().split('\\n')\n",
    "    print(source_words[:20])\n",
    "\n",
    "    # Make source language tokenizer. UNK token is 1\n",
    "    source_tokenizer = Tokenizer(num_words=len(source_words), oov_token=1)\n",
    "    source_tokenizer.fit_on_texts(source_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mQ3dc8c959X9"
   },
   "source": [
    "### French word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "er1SfE2L59X-",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if USE_WORD_LIST:\n",
    "    # Word list for target language\n",
    "    target_words = []\n",
    "\n",
    "    # Load target language word list and convert characters to lower\n",
    "    df = pd.read_csv('datasets/target_dictionary.tsv', sep='\\t', keep_default_na=False)['ortho'].tolist()\n",
    "    target_words = [w.lower() for w in df]\n",
    "    print(target_words[:20])\n",
    "\n",
    "    # Make target language tokenizer. UNK token is 1\n",
    "    target_tokenizer = Tokenizer(num_words=len(target_words), oov_token=1)\n",
    "    target_tokenizer.fit_on_texts(target_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fGvPrVH59X-"
   },
   "source": [
    "### English-to-French translation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1227,
     "status": "ok",
     "timestamp": 1617645153161,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "TB6PfTXi59X_"
   },
   "outputs": [],
   "source": [
    "SOS_TOKEN = '<sos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "UNK_TOKEN = '<unk>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2087,
     "status": "ok",
     "timestamp": 1617658454421,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "QktrLcW_59X_"
   },
   "outputs": [],
   "source": [
    "source_sentences = []\n",
    "target_sentences_input = []\n",
    "target_sentences_output = []\n",
    "\n",
    "lines = []\n",
    "with open('datasets/translation.txt') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "\n",
    "# Number of sentences to use\n",
    "NUM_LINES = 50_000\n",
    "count = 0\n",
    "last_sentence = None\n",
    "# Parse the source/target sentences and remove duplicates\n",
    "for line in lines:\n",
    "    source_sentence, target_sentence, _ = line.split('\\t')\n",
    "    if last_sentence is not None and last_sentence == source_sentence:\n",
    "        continue\n",
    "    last_sentence = source_sentence\n",
    "    source_sentences.append(source_sentence)\n",
    "    target_sentences_input.append(SOS_TOKEN + ' ' + target_sentence)\n",
    "    target_sentences_output.append(target_sentence + ' ' + EOS_TOKEN)\n",
    "    count += 1\n",
    "    if count >= NUM_LINES:\n",
    "        break\n",
    "\n",
    "print(len(source_sentences))\n",
    "print(source_sentences[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1372,
     "status": "ok",
     "timestamp": 1617636497205,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "XZz2SbGL59YA"
   },
   "outputs": [],
   "source": [
    "MAX_NUM_SOURCE_WORDS = 50_000\n",
    "MAX_NUM_TARGET_WORDS = 50_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1843,
     "status": "ok",
     "timestamp": 1617658460170,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "2nSO7NXv59YA"
   },
   "outputs": [],
   "source": [
    "if not USE_WORD_LIST:\n",
    "    # Make source language tokenizer\n",
    "    source_tokenizer = Tokenizer(num_words=MAX_NUM_SOURCE_WORDS)\n",
    "    source_tokenizer.fit_on_texts(source_sentences)\n",
    "\n",
    "    # Make destination language tokenizer\n",
    "    # Don't filter '<' and '>' because they're used for SOS and EOS tokens\n",
    "    target_tokenizer = Tokenizer(num_words=MAX_NUM_TARGET_WORDS, filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n",
    "    target_tokenizer.fit_on_texts(target_sentences_input + target_sentences_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build word dictionaries\n",
    "source_dict = source_tokenizer.word_index\n",
    "NUM_SOURCE_WORDS = len(source_dict)+1\n",
    "\n",
    "target_dict = target_tokenizer.word_index\n",
    "NUM_TARGET_WORDS = len(target_dict)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1129,
     "status": "ok",
     "timestamp": 1617645169075,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "WhUgFY-H59YA",
    "outputId": "bfc18d06-9f65-48ea-ec50-8348ce9d11b8"
   },
   "outputs": [],
   "source": [
    "print(len(source_dict))\n",
    "print(len(target_dict))\n",
    "# print(source_dict)\n",
    "# print(target_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3804,
     "status": "ok",
     "timestamp": 1617658463987,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "gIrdpn_w59YB"
   },
   "outputs": [],
   "source": [
    "# Tokenize sentences to sequences\n",
    "source_sequences = source_tokenizer.texts_to_sequences(source_sentences)\n",
    "target_sequences_input = target_tokenizer.texts_to_sequences(target_sentences_input)\n",
    "target_sequences_output = target_tokenizer.texts_to_sequences(target_sentences_output)\n",
    "\n",
    "# Reverse source sequnces\n",
    "if REVERSE_INPUT_SEQUENCES:\n",
    "    for sequence in source_sequences:\n",
    "        sequence.reverse()\n",
    "\n",
    "max_source_len = max([len(sequence) for sequence in source_sequences])\n",
    "max_target_len = max([len(sequence) for sequence in target_sequences_output])\n",
    "\n",
    "encoder_input_sequences = pad_sequences(source_sequences, maxlen=max_source_len, padding='pre')\n",
    "decoder_input_sequences = pad_sequences(target_sequences_input, maxlen=max_target_len, padding='post')\n",
    "decoder_output_sequences = pad_sequences(target_sequences_output, maxlen=max_target_len, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 442,
     "status": "ok",
     "timestamp": 1617636596081,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "fuJm2exi59YB",
    "outputId": "520a7de9-2efd-4852-d7d7-46efeed83517"
   },
   "outputs": [],
   "source": [
    "print(max_source_len)\n",
    "print(max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 613,
     "status": "ok",
     "timestamp": 1617543204556,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "0C72MCIp59YB",
    "outputId": "823b824f-5f04-4fbe-ffd5-d6dbb29ab67f"
   },
   "outputs": [],
   "source": [
    "for i in range(10000, 10005):\n",
    "    print(source_sentences[i])\n",
    "    print(str(encoder_input_sequences[i]))\n",
    "    print(target_sentences_input[i])\n",
    "    print(str(decoder_input_sequences[i]))\n",
    "    print(target_sentences_output[i])\n",
    "    print(str(decoder_output_sequences[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pRvk2e3t59YC"
   },
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xg5F2_T_59YC"
   },
   "source": [
    "The most common model for neural machine translation (NMT) is the sequence-to-sequence model which is a combination of recurrent neural network and encoder-decoder architectures. The enocder maps variable-length input sequence to a fixed-length representation, and the decoder uses that representation to generate an output sequence one word at a time.\n",
    "\n",
    "This type of model is used by Google for its translation service (https://arxiv.org/pdf/1609.08144.pdf, https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 604,
     "status": "ok",
     "timestamp": 1617636603406,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "AUkD9YhW59YC"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 765,
     "status": "ok",
     "timestamp": 1617658467380,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "8rCUvgLu59YD"
   },
   "outputs": [],
   "source": [
    "# Number of tokens is expected to be 1 greater than the number of words\n",
    "n_encoder_tokens = len(source_dict)+1\n",
    "n_decoder_tokens = len(target_dict)+1\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "LSTM_DIM = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNLRCJ3F59YE"
   },
   "source": [
    "### Embedding layer\n",
    "\n",
    "Keras documentation: https://keras.io/api/layers/core_layers/embedding/\n",
    "\n",
    "Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "\n",
    "e.g. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "\n",
    "This layer can only be used as the first layer in a model.\n",
    "\n",
    "Arguments:\n",
    "- **input_dim**: Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "- **output_dim**: Integer. Dimension of the dense embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90wEs2lN59YE"
   },
   "source": [
    "### LSTM layer\n",
    "\n",
    "Keras documentation: https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "\n",
    "Long Short-Term Memory layer\n",
    "\n",
    "Arguments:\n",
    "- **units**: Positive integer, dimensionality of the output space.\n",
    "- **kernel_initializer**: Initializer for the kernel weights matrix\n",
    "- **return_sequences**: Boolean. Whether to return the last output in the output sequence, or the full sequence. Default: False.\n",
    "- **return_state**: Boolean. Whether to return the last state in addition to the output. Default: False.\n",
    "\n",
    "return_state returns the lstm output, last hidden state and last cell state and return_sequences returns hidden states for each step as the main output. Both flags can be used at the same time and will return all hidden states, last hidden state and last cell state.\n",
    "\n",
    "For more information on return_sequences and return_state read [this](https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haweG4_h59YF"
   },
   "source": [
    "### Layer initializers\n",
    "\n",
    "Keras documentation: https://keras.io/api/layers/initializers/\n",
    "\n",
    "Initializers define the way to set the initial random weights of Keras layers.\n",
    "\n",
    "Some of the available initializers:\n",
    "- RandomNormal\n",
    "- RandomUniform\n",
    "- Zeros\n",
    "- Ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a36PnqK59YG"
   },
   "source": [
    "### GloVe Embeddings\n",
    "\n",
    "Instead of learning the embeddings for english words during training, a pre-trained model called [GloVe](https://nlp.stanford.edu/projects/glove/) will be used. It has been trained on 6 billion words from [Wikipedia 2014](https://dumps.wikimedia.org/enwiki/latest/) and [Gigaworld 5](https://catalog.ldc.upenn.edu/LDC2011T07). The french word embeddings will be learned during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12760,
     "status": "ok",
     "timestamp": 1617636623686,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "dDKdyDm459YG"
   },
   "outputs": [],
   "source": [
    "embeddings_dict = {}\n",
    "with open('datasets/glove.6B.100d.txt', encoding=\"utf8\") as glove_file:\n",
    "    for line in glove_file:\n",
    "        record = line.split()\n",
    "        word = record[0]\n",
    "        word_embedding = np.asarray(record[1:], dtype='float32')\n",
    "        embeddings_dict[word] = word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 630,
     "status": "ok",
     "timestamp": 1617658471600,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "pHFjbvD059YH"
   },
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((n_encoder_tokens, EMBEDDING_DIM))\n",
    "for word, index in source_dict.items():\n",
    "    embedding_vector = embeddings_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1213,
     "status": "ok",
     "timestamp": 1617645208252,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "1-K-a9ml59YH",
    "outputId": "0a60a552-307f-4485-a96f-85264ff270c8"
   },
   "outputs": [],
   "source": [
    "print(embeddings_dict[\"hello\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3fkJkIw59YI"
   },
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1040,
     "status": "ok",
     "timestamp": 1617658474553,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "r5JXak2Q59YI"
   },
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(max_source_len,))\n",
    "encoder_embedding = Embedding(n_encoder_tokens, EMBEDDING_DIM, weights=[embedding_matrix], trainable=False, input_length=max_source_len)\n",
    "encoder_lstm = LSTM(\n",
    "    LSTM_DIM,\n",
    "    kernel_initializer = tf.keras.initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None),\n",
    "    return_state=True\n",
    ")\n",
    "\n",
    "encoder = encoder_embedding(encoder_inputs)\n",
    "encoder, state_h, state_c = encoder_lstm(encoder)\n",
    "\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVo-ojRe59YJ"
   },
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 987,
     "status": "ok",
     "timestamp": 1617658476692,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "JOBd8_VO59YJ"
   },
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(max_target_len,))\n",
    "decoder_embedding = Embedding(n_decoder_tokens, EMBEDDING_DIM)\n",
    "decoder_lstm = LSTM(\n",
    "    LSTM_DIM, \n",
    "    kernel_initializer = tf.keras.initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None),\n",
    "    return_sequences=True,\n",
    "    return_state=True\n",
    ")\n",
    "decoder_dense = Dense(n_decoder_tokens, activation='softmax')\n",
    "\n",
    "decoder = decoder_embedding(decoder_inputs)\n",
    "decoder, _, _ = decoder_lstm(decoder, initial_state=encoder_states)\n",
    "decoder = decoder_dense(decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVXX3WjA59YK"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible to use beam search during training by definig a custom loss function\n",
    "# https://towardsdatascience.com/advanced-keras-constructing-complex-custom-losses-and-metrics-c07ca130a618\n",
    "\n",
    "# TODO Make adaptive learning rate using keras.callbacks.LearningRateScheduler\n",
    "# https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/\n",
    "# TODO Use SGD optimizer with no momentum, start rate 0.7\n",
    "# https://keras.io/api/optimizers/sgd/\n",
    "\n",
    "OPTIMIZER = 'rmsprop'\n",
    "LOSS = 'categorical_crossentropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 661,
     "status": "ok",
     "timestamp": 1617658479460,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "jwqAmg1f59YK"
   },
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder)\n",
    "model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 620,
     "status": "ok",
     "timestamp": 1617658483597,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "lA3E3Pvs59YK",
    "outputId": "12d7c7f0-f92e-4567-cf0c-b3eec9ca55c9"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUuSblOS59YL"
   },
   "source": [
    "### Plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "executionInfo": {
     "elapsed": 755,
     "status": "ok",
     "timestamp": 1617524079681,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "CDeb0hL359YL",
    "outputId": "a0e87ac9-e4d8-44bb-c74a-4e004384541f"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='plot/model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-7lLngS59YL"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1324,
     "status": "ok",
     "timestamp": 1617636686142,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "rMe9QYkJ59YL",
    "outputId": "52132489-63be-4ce9-8ed8-2de30ee8e7ef"
   },
   "outputs": [],
   "source": [
    "print(encoder_input_sequences.shape)\n",
    "print(decoder_input_sequences.shape)\n",
    "print(decoder_output_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 670,
     "status": "ok",
     "timestamp": 1617645224200,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "GYX4be3m59YL"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2303,
     "status": "ok",
     "timestamp": 1617636694190,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "7WXyPc--59YM"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 736,
     "status": "ok",
     "timestamp": 1617658490459,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "C6IBM0ot59YO"
   },
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, encoder_input, decoder_input, decoder_output, n_classes, batch_size=32, shuffle=True):\n",
    "        self.encoder_input = encoder_input\n",
    "        self.decoder_input = decoder_input\n",
    "        self.decoder_output = decoder_output\n",
    "        self.n_classes = n_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    # Number of batches per epoch\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.encoder_input) / self.batch_size))\n",
    "\n",
    "    # Generate one batch\n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        X = [self.encoder_input[indexes], self.decoder_input[indexes]]\n",
    "        # Decoder outputs have to be one-hot-encoded\n",
    "        y = np.asarray([to_categorical(self.decoder_output[index], num_classes=self.n_classes) for index in indexes])\n",
    "        return X, y\n",
    "\n",
    "    # Update indexes for next epoch\n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(len(self.encoder_input))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 665,
     "status": "ok",
     "timestamp": 1617658492538,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "7Wunksq_59YO"
   },
   "outputs": [],
   "source": [
    "if USE_DATA_GENERATORS:\n",
    "    indexes = np.arange(len(encoder_input_sequences))\n",
    "    np.random.shuffle(indexes)\n",
    "\n",
    "    validation_size = int(np.floor(len(encoder_input_sequences) * VALIDATION_SPLIT))\n",
    "\n",
    "    train_indexes = indexes[:-validation_size]\n",
    "    validation_indexes = indexes[-validation_size:]\n",
    "\n",
    "    training_generator = DataGenerator(\n",
    "        encoder_input_sequences[train_indexes],\n",
    "        decoder_input_sequences[train_indexes],\n",
    "        decoder_output_sequences[train_indexes],\n",
    "        n_decoder_tokens,\n",
    "        BATCH_SIZE\n",
    "    )\n",
    "\n",
    "    validation_generator = DataGenerator(\n",
    "        encoder_input_sequences[validation_indexes],\n",
    "        decoder_input_sequences[validation_indexes],\n",
    "        decoder_output_sequences[validation_indexes],\n",
    "        n_decoder_tokens,\n",
    "        BATCH_SIZE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_DATA_GENERATORS:\n",
    "    # Decoder outputs need to be one-hot-encoded for the dense softmax layer to work\n",
    "    decoder_output = np.array([to_categorical(output, num_classes=n_decoder_tokens) for output in decoder_output_sequences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CMQe2zY59YP"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "executionInfo": {
     "elapsed": 754,
     "status": "ok",
     "timestamp": 1617549466014,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "qQAVp8g459YP",
    "outputId": "523783a0-d568-4998-91ff-f0fa79c9c556"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.plot(history.epoch, history.history['accuracy'])\n",
    "    plt.plot(history.epoch, history.history['val_accuracy'])\n",
    "    plt.legend(['Training accuracy', 'Validation accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = f'model/translator_{NUM_LINES}_{EPOCHS}_{BATCH_SIZE}_{OPTIMIZER}_{time.strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='model/checkpoint',\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "if USE_DATA_GENERATORS:\n",
    "    history = model.fit(\n",
    "        training_generator,\n",
    "        validation_data=validation_generator,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )\n",
    "else:\n",
    "    history = model.fit(\n",
    "        [encoder_input_sequences, decoder_input_sequences],\n",
    "        decoder_output,\n",
    "        validation_split=VALIDATION_SPLIT,\n",
    "        epochs=EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )\n",
    "\n",
    "# Save trained model, compilation and training data and history plot\n",
    "model.save(model_name)\n",
    "with open(f'{model_name}/training.txt', 'w') as f:\n",
    "    f.write(f'LINES:     \\t {NUM_LINES}\\n')\n",
    "    f.write(f'OPTIMIZER: \\t {OPTIMIZER}\\n')\n",
    "    f.write(f'LOSS:      \\t {LOSS}\\n')\n",
    "    f.write(f'EPOCHS:    \\t {EPOCHS}\\n')\n",
    "    f.write(f'BATCH:     \\t {BATCH_SIZE}\\n')\n",
    "    f.write(f'VALIDATION:\\t {VALIDATION_SPLIT}\\n')\n",
    "    f.write(f'REVERSED:  \\t {REVERSE_INPUT_SEQUENCES}\\n')\n",
    "plot_history(history)\n",
    "plt.savefig(f'{model_name}/history.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KFOyNxbb59YP"
   },
   "source": [
    "### Save / Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "igezKvsM59YQ"
   },
   "outputs": [],
   "source": [
    "# model.save_weights('model/translator_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9510,
     "status": "ok",
     "timestamp": 1617644405864,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "eLvAB9xQ59YQ",
    "outputId": "a8a0ad7d-ae1f-4529-d229-1ecf6a42288c"
   },
   "outputs": [],
   "source": [
    "# model.save('model/translator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-on90rt59YQ"
   },
   "outputs": [],
   "source": [
    "# model.load_weights('model/translator_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "executionInfo": {
     "elapsed": 649,
     "status": "error",
     "timestamp": 1617644387751,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "FgTyMq8t59YQ",
    "outputId": "0436d2ac-2e44-4b89-96a1-00da86bd3f68"
   },
   "outputs": [],
   "source": [
    "# model = keras.models.load_model('model/translator')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MGBcVHGk59YR"
   },
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnWJ8jVu59YR"
   },
   "source": [
    "To predict outputs a separate model is needed because the previous decoder output has to be passed in as an input for the next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1236,
     "status": "ok",
     "timestamp": 1617659310150,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "uzftVvmd59YR"
   },
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_h = Input(shape=(LSTM_DIM,))\n",
    "decoder_state_c = Input(shape=(LSTM_DIM,))\n",
    "decoder_states = [decoder_state_h, decoder_state_c]\n",
    "\n",
    "decoder_input_word = Input(shape=(1,))\n",
    "\n",
    "inference_decoder = decoder_embedding(decoder_input_word)\n",
    "inference_decoder, state_h, state_c = decoder_lstm(inference_decoder, initial_state=decoder_states)\n",
    "inference_decoder = decoder_dense(inference_decoder)\n",
    "\n",
    "inference_states = [state_h, state_c]\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_input_word] + decoder_states,\n",
    "    [inference_decoder] + inference_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 801,
     "status": "ok",
     "timestamp": 1617644427395,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "PpaRHpXv59YR",
    "outputId": "98507c8d-f327-470a-b8fc-f017f7ad48c5"
   },
   "outputs": [],
   "source": [
    "encoder_model.summary()\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eW6pWiEe59YR"
   },
   "outputs": [],
   "source": [
    "plot_model(decoder_model, to_file='plot/inference_model.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bz7t6J5o59YS"
   },
   "source": [
    "### Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 803,
     "status": "ok",
     "timestamp": 1617659315107,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "SVQ7bK9q59YS"
   },
   "outputs": [],
   "source": [
    "reverse_source_dict = { v:k for k,v in source_dict.items() }\n",
    "reverse_target_dict = { v:k for k,v in target_dict.items() }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 875,
     "status": "ok",
     "timestamp": 1617659315545,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "DOtrlVHT59YS"
   },
   "outputs": [],
   "source": [
    "def sentence_to_sequence(sentence):\n",
    "    sequence = source_tokenizer.texts_to_sequences([sentence])\n",
    "    sequence.reverse()\n",
    "    sequence = pad_sequences([sequence], maxlen=max_source_len, padding='pre')\n",
    "    return sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1052,
     "status": "ok",
     "timestamp": 1617659316063,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "cR07SmQF59YS"
   },
   "outputs": [],
   "source": [
    "def sequence_to_sentence(sequence):\n",
    "    SOS_ID = target_dict[SOS_TOKEN]\n",
    "    EOS_ID = target_dict[EOS_TOKEN]\n",
    "    words = []\n",
    "    for i in sequence:\n",
    "        if i == SOS_ID:\n",
    "            continue\n",
    "        elif i == EOS_ID:\n",
    "            break\n",
    "        elif i > 0:\n",
    "            words.append(reverse_target_dict[i])\n",
    "        else:\n",
    "            words.append(UNK_TOKEN)\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 831,
     "status": "ok",
     "timestamp": 1617659317906,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "tW1Bi-DI59YS"
   },
   "outputs": [],
   "source": [
    "def predict_sequence(source_sequence):\n",
    "    # Predict the encoder result directly\n",
    "    current_state = encoder_model.predict(source_sequence)\n",
    "    \n",
    "    # Predict ouput words one at a time until <eos> token or max_target_len\n",
    "    EOS_ID = target_dict[EOS_TOKEN]\n",
    "    \n",
    "    # Decoder_model expects a tensor as input\n",
    "    decoder_input = np.zeros((1, 1))\n",
    "    decoder_input[0, 0] = target_dict[SOS_TOKEN]\n",
    "    target_sequence = []\n",
    "    \n",
    "    for _ in range(max_target_len):\n",
    "        dense_outputs, state_h, state_c = decoder_model.predict([decoder_input] + current_state)\n",
    "        token = np.argmax(dense_outputs[0, 0, :])\n",
    "        if token == EOS_ID:\n",
    "            break\n",
    "        else:\n",
    "            target_sequence.append(token)\n",
    "        \n",
    "        # Replace decoder inputs to last generated token and states\n",
    "        decoder_input[0, 0] = token\n",
    "        current_state = [state_h, state_c]\n",
    "    return target_sequence\n",
    "\n",
    "def predict(source_sequence):\n",
    "    target_sequence = predict_sequence(source_sequence)\n",
    "    return sequence_to_sentence(target_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVafMsk559YS"
   },
   "source": [
    "### Beam Search\n",
    "\n",
    "The beam search algorithm is used as an improvement to the greedy search algorithm for determining the next word in the sequence. It works by taking the k best options at each step and then using those for all next steps. The best options are evaluated by maximizing the average log probability of each word in the output. For k=1 this is equivalent to the greedy algorithm. Increasing the k value gives better results but also increases processing time exponentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 614,
     "status": "ok",
     "timestamp": 1617659319407,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "gbeOSGUz59YT"
   },
   "outputs": [],
   "source": [
    "def beam_search_predict_sequence(source_sequence, k=1):\n",
    "    encoder_output_state = encoder_model.predict([source_sequence])\n",
    "    decoder_input = np.zeros((1, 1))\n",
    "    \n",
    "    # (log(1), initial_sos_token, current_state)\n",
    "    k_beam = [(0, [target_dict[SOS_TOKEN]], encoder_output_state)]\n",
    "\n",
    "    EOS_ID = target_dict[EOS_TOKEN]\n",
    "    for i in range(max_target_len):\n",
    "        all_k_beams = []\n",
    "        for prob, predictions, state in k_beam:\n",
    "            if predictions[-1] == EOS_ID:\n",
    "                all_k_beams.append((prob, predictions, state))\n",
    "                continue\n",
    "            \n",
    "            decoder_input[0,0] = predictions[-1]\n",
    "            dense_outputs, state_h, state_c = decoder_model.predict([decoder_input] + state)\n",
    "            \n",
    "            # Get indices of top k predictions (last k when sorted)\n",
    "            top_k = dense_outputs[0,0].argsort()[-k:]\n",
    "\n",
    "            # Add to all possible candidates for k-beams\n",
    "            all_k_beams += [\n",
    "                (\n",
    "                    # We subtract the log because it's negative\n",
    "                    # Same as adding but sorting in reverse\n",
    "                    prob - np.log(dense_outputs[0,0,next_word]),\n",
    "                    # Append next word to the copy of existing list\n",
    "                    list(predictions)+[next_word],\n",
    "                    # Set state to new decoder state\n",
    "                    [state_h, state_c]\n",
    "                )\n",
    "                for next_word in top_k\n",
    "            ]\n",
    "\n",
    "        # Get k best tuples sorted by score/length\n",
    "        k_beam = sorted(all_k_beams, key = lambda t: t[0]/len(t[1]))[:k]\n",
    "\n",
    "    # Return best sequence\n",
    "    return k_beam[0][1]\n",
    "\n",
    "def beam_search_predict(source_sequence, k=1):\n",
    "    target_sequence = beam_search_predict_sequence(source_sequence, k)\n",
    "    return sequence_to_sentence(target_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tyg7ka6K59YT"
   },
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11659,
     "status": "ok",
     "timestamp": 1617659336507,
     "user": {
      "displayName": "Vladimir Vuksanovic",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhDVInHUBph5hXktgkKO4i2JKO7UL5JRrz87Y5touM=s64",
      "userId": "04006842609994023267"
     },
     "user_tz": -120
    },
    "id": "8vxdsYJM59YT",
    "outputId": "0fce8a15-152a-47bb-f16b-298d94c7f3ff"
   },
   "outputs": [],
   "source": [
    "indexes = np.random.randint(0, len(source_sentences), 10)\n",
    "\n",
    "sample_input_sequences = np.asarray(encoder_input_sequences[indexes])\n",
    "\n",
    "for i, index in enumerate(indexes):\n",
    "    print(\"source:\\t\" + source_sentences[index])\n",
    "    print(\"target:\\t\" + target_sentences_output[index][:-6])\n",
    "    print(\"greedy:\\t\" + predict(np.asarray([sample_input_sequences[i]])))\n",
    "    print(\"beam:\\t\" + beam_search_predict(np.asarray([sample_input_sequences[i]]), k=3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F0l_RcVj59YT"
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzLP9jZZ59YT"
   },
   "source": [
    "### Ground truth comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCGJ6eEF59YU"
   },
   "outputs": [],
   "source": [
    "def to_sequence(sentence):\n",
    "    sequences = source_tokenizer.texts_to_sequences([sentence])\n",
    "    if REVERSE_INPUT_SEQUENCES:\n",
    "        sequences[0].reverse()\n",
    "    sequences = pad_sequences(sequences, maxlen=max_source_len, padding='pre')\n",
    "#     print(sequences)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(to_sequence(\"i\")))\n",
    "print(beam_search_predict(to_sequence(\"i\"), k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(to_sequence(\"it's raining\")))\n",
    "print(beam_search_predict(to_sequence(\"it's raining\"), k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(to_sequence(\"what time is it?\")))\n",
    "print(beam_search_predict(to_sequence(\"what time is it?\"), k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(to_sequence(\"i am going to run home tomorrow\")))\n",
    "print(beam_search_predict(to_sequence(\"i am going to run home tomorrow\"), k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(to_sequence(\"he bought a book at the store\")))\n",
    "print(beam_search_predict(to_sequence(\"he bought a book at the store\"), k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict(to_sequence(\"he is a good swimmer\")))\n",
    "print(beam_search_predict(to_sequence(\"he is a good swimmer\"), k=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImG_ayqq59YU"
   },
   "source": [
    "### 2D Encoder hidden state PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9buyYthr59YU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ugvAMD5359YU"
   },
   "outputs": [],
   "source": [
    "sample_indexes = np.random.randint(0, len(source_sentences), 10)\n",
    "sample_sentences = np.array(source_sentences)[sample_indexes]\n",
    "\n",
    "source_sequences = source_tokenizer.texts_to_sequences(sample_sentences)\n",
    "if REVERSE_INPUT_SEQUENCES:\n",
    "    # Reverse source sequences\n",
    "    for sequence in source_sequences:\n",
    "        sequence.reverse()\n",
    "sample_sequences = pad_sequences(source_sequences, maxlen=max_source_len, padding='pre')\n",
    "\n",
    "sample_inputs = encoder_model.predict(sample_sequences)[0]\n",
    "\n",
    "# print(sample_sentences)\n",
    "# print(sample_inputs)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(sample_inputs)\n",
    "pca_inputs = pca.transform(sample_inputs)\n",
    "\n",
    "plt.scatter(pca_inputs[:,0], pca_inputs[:,1])\n",
    "for i in range(len(sample_sentences)):\n",
    "    plt.text(pca_inputs[i,0], pca_inputs[i, 1], sample_sentences[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bcq1whW159YU"
   },
   "source": [
    "### BLEU score\n",
    "\n",
    "BLEU (Bilingual Evaluation Understudy) is a score for comparing machine-translated text to one or more reference translations made by a human. Scoring usuall works on indiviual sentences and is then averaged on the entire text. Scores have a value between 0 and 1.\n",
    "\n",
    "For more info:\n",
    "- https://en.wikipedia.org/wiki/BLEU\n",
    "- https://machinelearningmastery.com/calculate-bleu-score-for-text-python/\n",
    "- https://towardsdatascience.com/bleu-bilingual-evaluation-understudy-2b4eab9bcfd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uu-PwXCQ59YV"
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = np.random.randint(0, len(source_sentences), 10)\n",
    "\n",
    "sample_input_sequences = np.asarray(encoder_input_sequences[indexes])\n",
    "sample_output_sequences = []\n",
    "sample_predictions = []\n",
    "sample_predictions_beam = []\n",
    "\n",
    "for i, index in enumerate(indexes):\n",
    "    target_sequence = sequence_to_sentence(target_tokenizer.texts_to_sequences([target_sentences_output[index]])[0]).split(' ')\n",
    "    prediction_sequence = predict(np.asarray([sample_input_sequences[i]])).split(' ')\n",
    "    prediction_sequence_beam = beam_search_predict(np.asarray([sample_input_sequences[i]]), k=3).split(' ')\n",
    "    \n",
    "    sample_output_sequences.append([target_sequence])\n",
    "    sample_predictions.append(prediction_sequence)\n",
    "    sample_predictions_beam.append(prediction_sequence_beam)\n",
    "    \n",
    "    print(f'source:\\t {source_sentences[index]}')\n",
    "    print(f'target:\\t {\" \".join(target_sequence)}')\n",
    "    print(f'predictions:')\n",
    "    print(f'greedy:\\t {\" \".join(prediction_sequence)}')\n",
    "    print(f'score:\\t {sentence_bleu([target_sequence], prediction_sequence)}')\n",
    "    print(f'beam:\\t {\" \".join(prediction_sequence_beam)}')\n",
    "    print(f'score:\\t {sentence_bleu([target_sequence], prediction_sequence_beam)}')\n",
    "    print()\n",
    "\n",
    "print(f'regular corpus score:\\t {corpus_bleu(sample_output_sequences, sample_predictions)}')\n",
    "print(f'beam corpus score:\\t {corpus_bleu(sample_output_sequences, sample_predictions_beam)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLz-6PUz59YV"
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rabl2a859YV"
   },
   "source": [
    "- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/pdf/1409.3215.pdf)\n",
    "- [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078.pdf)\n",
    "- [Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/pdf/1609.08144.pdf)\n",
    "- [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)\n",
    "- https://google.github.io/seq2seq/\n",
    "- https://keras.io/examples/nlp/lstm_seq2seq/\n",
    "- https://nlp.stanford.edu/projects/glove/"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Automatic_Translation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
