{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-25b41bf850ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download datasets\n",
    "\n",
    "Translation dataset used in original paper can be found [here](https://www.tensorflow.org/datasets/catalog/wmt14_translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make dataset directory if doesn't exist\n",
    "!!mkdir -p datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### curl\n",
    "\n",
    "Use curl to download files from the Internet\n",
    "\n",
    "`-o` option to set file name when downloaded\n",
    "\n",
    "`-O` option to keep remote file name\n",
    "\n",
    "output path can't be specified so manually cd to wanted directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current',\n",
       " '                                 Dload  Upload   Total   Spent    Left  Speed',\n",
       " '',\n",
       " '  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0',\n",
       " '100  5839  100  5839    0     0  11916      0 --:--:-- --:--:-- --:--:-- 11916']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download english language word list\n",
    "!!cd datasets && curl -o english_words.txt https://gist.githubusercontent.com/deekayen/4148741/raw/98d35708fa344717d8eee15d11987de6c8e26d7d/1-1000.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download french language word list\n",
    "!!cd datasets && curl -O http://www.lexique.org/databases/Lexique383/Lexique383.zip\n",
    "!!cd datasets && unzip Lexique383.zip\n",
    "!!mv datasets/Lexique383.tsv datasets/french_words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download english-to-french translation dataset\n",
    "!!cd datasets && curl -O http://www.manythings.org/anki/fra-eng.zip\n",
    "!!cd datasets && unzip fra-eng.zip\n",
    "!!rm datasets/_about.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Sentences from the source and destination languages are converted to token sequences with special tokens representing words not in dicionary (UNK) and end of sequence (EOS). Source language sequences are also reversed for better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word list for source language\n",
    "input_words = []\n",
    "\n",
    "# load source language word list and convert characters to lower\n",
    "with open('datasets/english_words.txt') as f:\n",
    "    input_words = f.read().lower().split('\\n')\n",
    "# print(input_words)\n",
    "\n",
    "# Make source language tokenizer. UNK token is 1\n",
    "source_tokenizer = Tokenizer(num_words=len(input_words), oov_token=1)\n",
    "source_tokenizer.fit_on_texts(input_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### French word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word list for destination language\n",
    "target_words = []\n",
    "\n",
    "# Target language\n",
    "# load target language word list and convert characters to lower\n",
    "# with open('datasets/french_words.txt') as f:\n",
    "#     target_words = f.read().lower().split('\\n')\n",
    "# print(target_words[0])\n",
    "df = pd.read_csv('datasets/french_words.txt')\n",
    "print(df.head())\n",
    "\n",
    "# # Make target language tokenizer. UNK token is 1\n",
    "# target_tokenizer = Tokenizer(num_words=len(target_words), oov_token=1)\n",
    "# target_tokenizer.fit_on_texts(target_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English-to-French translation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences = []\n",
    "target_sentences = []\n",
    "\n",
    "lines = []\n",
    "with open('datasets/fra.txt') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[:-1]:\n",
    "    input_sentence, target_sentence, _ = line.split('\\t')\n",
    "    input_sentences.append(input_sentence)\n",
    "    target_sentences.append(target_sentence)\n",
    "\n",
    "# Tokenize sentences to sequences\n",
    "input_sequences = source_tokenizer.texts_to_sequences(input_sentences)\n",
    "target_sequences = target_tokenizer.texts_to_sequences(target_sentences)\n",
    "\n",
    "for i in range(0, 100):\n",
    "    print(input_sequences[i] + ' ' + target_sequences[i])\n",
    "    \n",
    "# TODO reverse source sequnces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer\n",
    "\n",
    "Keras documentation: https://keras.io/api/layers/core_layers/embedding/\n",
    "\n",
    "Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "\n",
    "e.g. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "\n",
    "This layer can only be used as the first layer in a model.\n",
    "\n",
    "Arguments:\n",
    "- **input_dim**: Integer. Size of the vocabulary, i.e. maximum integer index + 1.\n",
    "- **output_dim**: Integer. Dimension of the dense embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM layer\n",
    "\n",
    "Keras documentation: https://keras.io/api/layers/recurrent_layers/lstm/\n",
    "\n",
    "Long Short-Term Memory layer\n",
    "\n",
    "Arguments:\n",
    "- **units**: Positive integer, dimensionality of the output space.\n",
    "- **kernel_initializer**: Initializer for the kernel weights matrix\n",
    "- **return_sequences**: Boolean. Whether to return the last output. in the output sequence, or the full sequence. Default: False.\n",
    "- **return_state**: Boolean. Whether to return the last state in addition to the output. Default: False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer initializers\n",
    "\n",
    "Keras documentation: https://keras.io/api/layers/initializers/\n",
    "\n",
    "Initializers define the way to set the initial random weights of Keras layers.\n",
    "\n",
    "Some of the available initializers:\n",
    "- RandomNormal\n",
    "- RandomUniform\n",
    "- Zeros\n",
    "- Ones\n",
    "\n",
    "Original paper uses uniform distribution in range \\[-0.08, 0.08\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of tokens is len of input_words + UNK + 1\n",
    "n_encoder_tokens = len(input_words)+2\n",
    "\n",
    "# Number of tokens is len of target_words + UNK + 1\n",
    "num_decoder_tokens = len(target_words)+2\n",
    "\n",
    "# Output dim (from paper)\n",
    "lstm_dim = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(num_encoder_tokens, lstm_dim)(encoder_inputs)\n",
    "x, state_h, state_c = LSTM(\n",
    "    lstm_dim,\n",
    "    kernel_initilizer = tf.keras.initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None),\n",
    "    return_state=True\n",
    ")(x)\n",
    "\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None,))\n",
    "x = Embedding(num_decoder_tokens, lstm_dim)(decoder_inputs)\n",
    "x = LSTM(\n",
    "    lstm_dim, \n",
    "    kernel_initilizer = tf.keras.initializers.RandomUniform(minval=-0.08, maxval=0.08, seed=None),\n",
    "    return_sequences=True\n",
    ")(x, initial_state=encoder_states)\n",
    "decoder_outputs = Dense(num_decoder_tokens, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO Use separate models for encoder and decoder\n",
    "\n",
    "TODO Use beam search\n",
    "\n",
    "Beam search:\n",
    "- https://machinelearningmastery.com/beam-search-decoder-natural-language-processing/\n",
    "- https://towardsdatascience.com/boosting-your-sequence-generation-performance-with-beam-search-language-model-decoding-74ee64de435a\n",
    "- https://medium.com/machine-learning-bites/deeplearning-series-sequence-to-sequence-architectures-4c4ca89e5654"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# TODO Make adaptive learning rate using keras.callbacks.LearningRateScheduler\n",
    "# https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/\n",
    "# TODO Use SGD optimizer with no momentum, start rate 0.7\n",
    "# https://keras.io/api/optimizers/sgd/\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO 2-dimensional PCA of hidden state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
